{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "k35Swp-8KoKs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: open-spiel in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.5)\n",
      "Requirement already satisfied: pip>=20.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from open-spiel) (24.2)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from open-spiel) (24.2.0)\n",
      "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from open-spiel) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.21.5 in /home/codespace/.local/lib/python3.12/site-packages (from open-spiel) (2.1.1)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /home/codespace/.local/lib/python3.12/site-packages (from open-spiel) (1.14.1)\n",
      "Requirement already satisfied: ml-collections>=0.1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from open-spiel) (1.0.0)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.12/site-packages (from ml-collections>=0.1.1->open-spiel) (1.16.0)\n",
      "Requirement already satisfied: PyYAML in /home/codespace/.local/lib/python3.12/site-packages (from ml-collections>=0.1.1->open-spiel) (6.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /home/codespace/.local/lib/python3.12/site-packages (2.4.1+cpu)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Needed if running on Colab\n",
    "!pip3 install open-spiel\n",
    "!pip3 install torch\n",
    "!pip3 install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "O1yAh0sTKs3K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5220:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "/home/codespace/.local/lib/python3.12/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from go_search_problem import GoProblem, GoState\n",
    "from heuristic_go_problems import GoProblemLearnedHeuristic, GoProblemSimpleHeuristic\n",
    "from agents import GreedyAgent, RandomAgent, MCTSAgent, GameAgent, AlphaBetaAgent, IterativeDeepeningAgent\n",
    "import matplotlib.pyplot as plt\n",
    "from go_utils import create_go_game\n",
    "from tqdm import tqdm\n",
    "from game_runner import run_many\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "state = GoState(create_go_game(5))\n",
    "state.apply_action(1)\n",
    "print(state.internal_state.move_number())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6XNzHOq6QCQD"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path: str):\n",
    "    with open(path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset\n",
    "\n",
    "dataset_5x5 = load_dataset('dataset_5x5.pkl')\n",
    "# dataset_9x9 = load_dataset('9x9_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5fAQlSAOLXoj"
   },
   "outputs": [],
   "source": [
    "def save_model(path: str, model):\n",
    "    \"\"\"\n",
    "    Save model to a file\n",
    "    Input:\n",
    "        path: path to save model to\n",
    "        model: Pytorch model to save\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "def load_model(path: str, model):\n",
    "    \"\"\"\n",
    "    Load model from file\n",
    "\n",
    "    Note: you still need to provide a model (with the same architecture as the saved model))\n",
    "\n",
    "    Input:\n",
    "        path: path to load model from\n",
    "        model: Pytorch model to load\n",
    "    Output:\n",
    "        model: Pytorch model loaded from file\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOzaAXYrM4d3"
   },
   "source": [
    "# Task 1: Convert GameState to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1hbg6LkAMrZW"
   },
   "outputs": [],
   "source": [
    "def get_features(game_state: GoState):\n",
    "    \"\"\"\n",
    "    Map a game state to a list of features.\n",
    "\n",
    "    Some useful functions from game_state include:\n",
    "        game_state.size: size of the board\n",
    "        get_pieces_coordinates(player_index): get coordinates of all pieces of a player (0 or 1)\n",
    "        get_pieces_array(player_index): get a 2D array of pieces of a player (0 or 1)\n",
    "        \n",
    "        get_board(): get a 2D array of the board with 4 channels (player 0, player 1, empty, and player to move). 4 channels means the array will be of size 4 x n x n\n",
    "    \n",
    "        Descriptions of these methods can be found in the GoState\n",
    "\n",
    "    Input:\n",
    "        game_state: GoState to encode into a fixed size list of features\n",
    "    Output:\n",
    "        features: list of features\n",
    "    \"\"\"\n",
    "    board_size = game_state.size\n",
    "    \n",
    "    # TODO: Encode game_state into a list of features\n",
    "    features = []\n",
    "    #Get the current state\n",
    "    #get board\n",
    "    #\n",
    "    board = game_state.get_board()\n",
    "    \n",
    "    player_0 = list(board[0].flatten())\n",
    "    player_1 = list(board[1].flatten())\n",
    "    empty = list(board[2].flatten())\n",
    "    \n",
    "    player_to_move = (board[3][0][0])\n",
    "   \n",
    "    #could consider adding extra features\n",
    "    features = player_0+ player_1 + empty\n",
    "    features.append(player_to_move)\n",
    "\n",
    "    player_0_count = sum(player_0)\n",
    "    features.append(player_0_count)\n",
    "    player_1_count = sum(player_1)\n",
    "    features.append(player_1_count)\n",
    "    empty_count = sum(empty)\n",
    "    features.append(empty_count)\n",
    "\n",
    "    if player_to_move == 0:\n",
    "        count_diff = player_0_count - player_1_count\n",
    "    else:\n",
    "        count_diff = player_1_count - player_0_count\n",
    "    features.append(count_diff)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2cpr86wH8W3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoState(komi=0.5, to_play=B, history.size()=0)\n",
      "\n",
      " 5 +++++\n",
      " 4 +++++\n",
      " 3 +++++\n",
      " 2 +++++\n",
      " 1 +++++\n",
      "   ABCDE\n",
      "\n",
      "features [np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(25.0), np.float64(0.0)]\n",
      "Action # 11\n",
      "Game Result -1.0\n",
      "features len 80\n"
     ]
    }
   ],
   "source": [
    "# Print information about first data point\n",
    "data_point = dataset_5x5[0]\n",
    "\n",
    "features =(get_features(data_point[0]))\n",
    "\n",
    "action = data_point[1]\n",
    "result = data_point[2]\n",
    "print(data_point[0])\n",
    "print(\"features\", features)\n",
    "print(\"Action #\", action)\n",
    "print(\"Game Result\", result)\n",
    "print(\"features len\" , len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI86jYgcOfHC"
   },
   "source": [
    "# Task 2: Supervised Learning of a Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "boPRx0o5Bqq9"
   },
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "      super(ValueNetwork, self).__init__()\n",
    "\n",
    "      # TODO: What should the output size of a Value function be?\n",
    "      output_size = 1\n",
    "\n",
    "      # TODO: Add more layers, non-linear functions, etc.\n",
    "      #self.linear = nn.Linear(input_size, output_size)\n",
    "      self.in_layer = nn.Linear(input_size,100)\n",
    "        \n",
    "      self.layer_1 = nn.Linear(100,90)\n",
    "\n",
    "      self.layer_2 = nn.Linear(90,70)\n",
    "\n",
    "      self.layer_3 = nn.Linear(70,50)\n",
    "\n",
    "      self.out_layer = nn.Linear(50,1)\n",
    "\n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "      self.tanh = nn.Tanh()\n",
    "      self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "      \"\"\"\n",
    "      Run forward pass of network\n",
    "\n",
    "      Input:\n",
    "        x: input to network\n",
    "      Output:\n",
    "        output of network\n",
    "      \"\"\"\n",
    "      # TODO: Update as more layers are added\n",
    "      \n",
    "      a = self.in_layer(x)\n",
    "      b = self.tanh(a)\n",
    "\n",
    "      c = self.layer_1(b)\n",
    "      d = self.relu(c)\n",
    "\n",
    "      e = self.layer_2(d)\n",
    "      f = self.relu(e)\n",
    "\n",
    "      g = self.layer_3(f)\n",
    "      h = self.relu(g)\n",
    "\n",
    "      i = self.out_layer(h)\n",
    "      j = self.sigmoid(i)\n",
    "      return (j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "83a6vGLqB4E7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted Value tensor([0.5140], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This will not produce meaningful outputs until trained, but you can test for syntax errors\n",
    "\n",
    "features_tensor = torch.Tensor(features)\n",
    "\n",
    "value_net = ValueNetwork(len(features))\n",
    "print(\"predicted Value\", value_net(features_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Rq8CokTvOyrI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiasbronner/Desktop/cs410/cs410_env/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_test lossin in epoch 0 is  244.89709839906098\n",
      "average_test lossin in epoch 1 is  245.89332103780202\n",
      "average_test lossin in epoch 2 is  245.0771660546067\n",
      "average_test lossin in epoch 3 is  244.6143265086181\n",
      "average_test lossin in epoch 4 is  244.39116888035804\n",
      "average_test lossin in epoch 5 is  244.25442959156493\n",
      "average_test lossin in epoch 6 is  244.1481239174706\n",
      "average_test lossin in epoch 7 is  244.0511907615367\n",
      "average_test lossin in epoch 8 is  243.974725548273\n",
      "average_test lossin in epoch 9 is  243.91335612646353\n"
     ]
    }
   ],
   "source": [
    "def train_value_network(dataset, num_epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a value network on the provided dataset.\n",
    "\n",
    "    Input:\n",
    "        dataset: list of (state, action, result) tuples\n",
    "        num_epochs: number of epochs to train for\n",
    "        learning_rate: learning rate for gradient descent\n",
    "    Output:\n",
    "        model: trained model\n",
    "    \"\"\"\n",
    "    # Make sure dataset is shuffled for better performance\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    train_data, test_data = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "        # You may find it useful to create train/test sets to better track performance/overfit/underfit\n",
    "    # TODO: Create model\n",
    "    #Make sure this size is right\n",
    "    model = ValueNetwork(len(get_features(GoState(create_go_game(5)))))\n",
    "\n",
    "    # TODO: Specify Loss Function\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    # You can use Adam, which is stochastic gradient descent with ADAptive Momentum\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    batch_size = 32\n",
    "    batch_loss = 0.0\n",
    "    batch_counter = 0 \n",
    "   \n",
    "    for epoch in range(num_epochs):\n",
    "        test_loss = 0\n",
    "        for data_point in train_data:\n",
    "            state = data_point[0]\n",
    "            features = get_features(state)\n",
    "            features_tensor = torch.tensor(features,dtype=torch.float32)\n",
    "\n",
    "            # TODO: What should the desired output of the value network be?\n",
    "            # Note: You will have to convert the label to a torch tensor to use with torch's loss functions\n",
    "            label = torch.tensor(data_point[2],dtype=torch.float32)\n",
    "\n",
    "            # TODO: Get model prediction of value\n",
    "            prediction = model.forward(features_tensor) \n",
    "\n",
    "            # TODO: Compute Loss for data point\n",
    "            loss = loss_function(prediction,label)\n",
    "            \n",
    "            batch_loss += loss\n",
    "            batch_counter += 1\n",
    "            if batch_counter % batch_size == 0:\n",
    "                # Call backward to run backward pass and compute gradients\n",
    "                batch_loss.backward()\n",
    "\n",
    "                # Run gradient descent step with optimizer\n",
    "                optimizer.step()\n",
    "\n",
    "                # Reset gradient for next batch\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss = 0.0\n",
    "                batch_counter = 0 \n",
    "            \n",
    "        for data_point in test_data:\n",
    "            state = data_point[0]\n",
    "            features = get_features(state)\n",
    "            features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "            label = torch.tensor(data_point[1])\n",
    "            prediction = model.forward(features_tensor)\n",
    "\n",
    "            test_loss += loss_function(prediction, label).item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_data)\n",
    "        print(\"average_test lossin in epoch \" + str(epoch) + \" is \", avg_test_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "value_model = train_value_network(dataset_5x5, 10, 1e-5)\n",
    "save_model(\"value_model.pt\", value_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekP8mzDaTOUM"
   },
   "source": [
    "## Comparing Learned Value function against other Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWl3dLOnTbiD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Agent IterativeDeepneing + Simple Heuristic\n",
      "Learned Agent GreedyAgent + Learned Heuristic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.moves 0\n",
      "self.moves 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:33<02:15, 33.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.moves 18\n",
      "self.moves 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [01:08<01:43, 34.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.moves 22\n",
      "self.moves 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:52<01:17, 38.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.moves 20\n",
      "self.moves 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [02:34<00:40, 40.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.moves 18\n",
      "self.moves 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:10<00:00, 38.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1: GreedyAgent + Learned Heuristic Score: 10.0\n",
      "Agent 2: IterativeDeepneing + Simple Heuristic Score: -10.0\n",
      "Agent 1: GreedyAgent + Learned Heuristic Score with Black (first move): 5.0\n",
      "Agent 2: IterativeDeepneing + Simple Heuristic Score with Black (first move): -5.0\n",
      "Agent 1: GreedyAgent + Learned Heuristic Average Duration: 0.001136414081183912\n",
      "Agent 2: IterativeDeepneing + Simple Heuristic Average Duration: 0.998583859134767\n",
      "Agent 1: GreedyAgent + Learned Heuristic Longest Duration: 0.003401041030883789\n",
      "Agent 2: IterativeDeepneing + Simple Heuristic Longest Duration: 1.0001869201660156\n",
      "Agent 1: GreedyAgent + Learned Heuristic Average Time Remaining: 33.879002118110655\n",
      "Agent 2: IterativeDeepneing + Simple Heuristic Average Time Remaining: 15.029584789276123\n",
      "Agent 1: GreedyAgent + Learned Heuristic Min Time Remaining: 25.98527193069458\n",
      "Agent 2: IterativeDeepneing + Simple Heuristic Min Time Remaining: 14.999572038650513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10.0, -10.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class GoProblemLearnedHeuristic(GoProblem):\n",
    "    def __init__(self, model=None, state=None):\n",
    "        super().__init__(state=state)\n",
    "        self.model = model\n",
    "        \n",
    "    def __call__(self, model=None):\n",
    "        \"\"\"\n",
    "        Use the model to compute a heuristic value for a given state.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def encoding(self, state):\n",
    "        \"\"\"\n",
    "        Get encoding of state (convert state to features)\n",
    "        Note, this may call get_features() from Task 1. \n",
    "\n",
    "        Input:\n",
    "            state: GoState to encode into a fixed size list of features\n",
    "        Output:\n",
    "            features: list of features\n",
    "        \"\"\"\n",
    "        # TODO: get encoding of state (convert state to features)\n",
    "        features = get_features(state)\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "        return features_tensor\n",
    "\n",
    "    def heuristic(self, state, player_index):\n",
    "        \"\"\"\n",
    "        Return heuristic (value) of current state\n",
    "\n",
    "        Input:\n",
    "            state: GoState to encode into a fixed size list of features\n",
    "            player_index: index of player to evaluate heuristic for\n",
    "        Output:\n",
    "            value: heuristic (value) of current state\n",
    "        \"\"\"\n",
    "        # TODO: Compute heuristic (value) of current state\n",
    "        value = 0\n",
    "\n",
    "        features = self.encoding(state)\n",
    "        value = self.model.forward(features)\n",
    "        # Note, your agent may perform better if you force it not to pass\n",
    "        # (i.e., don't select action #25 on a 5x5 board unless necessary)\n",
    "        return value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"Learned Heuristic\"\n",
    "\n",
    "\n",
    "def create_value_agent_from_model(value_net):\n",
    "    \"\"\"\n",
    "    Create agent object from saved model. This (or other methods like this) will be how your agents will be created in gradescope and in the final tournament.\n",
    "    \"\"\"\n",
    "\n",
    "    model_path = \"value_model.pt\"\n",
    "    # TODO: Update number of features for your own encoding size\n",
    "    feature_size = len(get_features(GoState(create_go_game(5))))\n",
    "    model = load_model(model_path, ValueNetwork(feature_size))\n",
    "    heuristic_search_problem = GoProblemLearnedHeuristic(model)\n",
    "\n",
    "    # TODO: Try with other heuristic agents (IDS/AB/Minimax)\n",
    "    learned_agent = GreedyAgent(heuristic_search_problem)\n",
    "    # learned_agent = RandomAgent()\n",
    "\n",
    "    return learned_agent\n",
    "\n",
    "learned_agent = create_value_agent_from_model(value_net)\n",
    "agent2 = IterativeDeepeningAgent(1)\n",
    "print(\"Greedy Agent\", agent2)\n",
    "print(\"Learned Agent\", learned_agent)\n",
    "\n",
    "run_many(learned_agent, agent2,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUUOOIYhRjoT"
   },
   "source": [
    "# Task 3: Supervised Learning of a Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dHgeNqBeBm3b"
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, board_size=5):\n",
    "      super(PolicyNetwork, self).__init__()\n",
    "    \n",
    "\n",
    "     \n",
    "\n",
    "      # TODO: Add more layers, non-linear functions, etc.\n",
    "      #self.linear = nn.Linear(input_size, output_size)\n",
    "      self.in_layer = nn.Linear(input_size,128)\n",
    "        \n",
    "      self.layer_1 = nn.Linear(128,50)\n",
    "\n",
    "      self.layer_2 = nn.Linear(50,32)\n",
    "\n",
    "      self.layer_3 = nn.Linear(32,26)\n",
    "\n",
    "      self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "      \"\"\"\n",
    "      Run forward pass of network\n",
    "\n",
    "      Input:\n",
    "        x: input to network\n",
    "      Output:\n",
    "        output of network\n",
    "      \"\"\"\n",
    "      # TODO: Update as more layers are added\n",
    "      \n",
    "      a = self.in_layer(x)\n",
    "      b = self.relu(a)\n",
    "\n",
    "      c = self.layer_1(b)\n",
    "      d = self.relu(c)\n",
    "\n",
    "      e = self.layer_2(d)\n",
    "      f = self.relu(e)\n",
    "\n",
    "      g = self.layer_3(f)\n",
    "     \n",
    "      return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "toR5q6qrBvUI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Action Probabilities tensor([ 0.0081, -0.0538,  0.0305,  0.0924,  0.1959, -0.0186, -0.0499, -0.0316,\n",
      "        -0.0877,  0.1153,  0.1518, -0.0040,  0.0746,  0.0237, -0.3385, -0.0298,\n",
      "         0.0241,  0.0460,  0.0482,  0.0562,  0.2286, -0.0640, -0.0027,  0.0121,\n",
      "        -0.1015,  0.1124], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This will not produce meaningful outputs until trained, but you can test for syntax errors\n",
    "features_tensor = torch.Tensor(features)\n",
    "policy_net = PolicyNetwork(len(features))\n",
    "print(\"Predicted Action Probabilities\", policy_net(features_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6-P_g6wRi-Y"
   },
   "outputs": [],
   "source": [
    "def train_policy_network(dataset, num_epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a policy network on the provided dataset.\n",
    "\n",
    "    Input:\n",
    "        dataset: list of (state, action, result) tuples\n",
    "        num_epochs: number of epochs to train for\n",
    "        learning_rate: learning rate for gradient descent\n",
    "    Output:\n",
    "        model: trained model\n",
    "    \"\"\"\n",
    "    random.shuffle(dataset)\n",
    "    train_data, test_data = train_test_split(dataset, test_size=test_size, random_state=42)\n",
    "\n",
    "    model = PolicyNetwork(len(get_features(GoState(create_go_game(5)))))\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # You can use Adam, which is stochastic gradient descent with ADAptive Momentum\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        for data_point in dataset:\n",
    "          \n",
    "            state = data_point[0]\n",
    "            features = get_features(state)\n",
    "            features_tensor = torch.tensor(features,dtype = torch.float32)\n",
    "\n",
    "            # TODO: What should the desired output of the value network be?\n",
    "            # Note: You will have to convert the label to a torch tensor to use with torch's loss functions\n",
    "            label = torch.tensor(data_point[1])\n",
    "            # TODO: Get model estimate of value\n",
    "            prediction = model.forward(features_tensor)\n",
    "\n",
    "            # TODO: Compute Loss for data point\n",
    "            loss = loss_function(prediction,label)\n",
    "\n",
    "            # Call backward to run backward pass and compute gradients\n",
    "            loss.backward()\n",
    "        \n",
    "            # Run gradient descent step with optimizer\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "    return model\n",
    "\n",
    "policy_net = train_policy_network(dataset_5x5, 10, 1e-4)\n",
    "save_model(\"policy_model.pt\", policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uU3ZxNNi-gWc"
   },
   "source": [
    "## Comparing Learned Policy against other Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNqXigG6-dHG"
   },
   "outputs": [],
   "source": [
    "class PolicyAgent(GameAgent):\n",
    "    def __init__(self, search_problem, model_path, board_size=5):\n",
    "        super().__init__()\n",
    "        self.search_problem = search_problem\n",
    "        self.model = load_model(model_path, PolicyNetwork(len(get_features(GoState(create_go_game(5)))),5))\n",
    "        self.board_size = board_size\n",
    "\n",
    "    def encoding(self, state):\n",
    "        # TODO: get encoding of state (convert state to features)\n",
    "        features = get_features(state)\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "        return features_tensor\n",
    "\n",
    "    def get_move(self, game_state, time_limit=1):\n",
    "      \"\"\"\n",
    "      Get best action for current state using self.model\n",
    "\n",
    "      Input:\n",
    "        game_state: current state of the game\n",
    "        time_limit: time limit for search (This won't be used in this agent)\n",
    "      Output:\n",
    "        action: best action to take\n",
    "      \"\"\"\n",
    "\n",
    "      # TODO: Select LEGAL Best Action predicted by model\n",
    "      # The top prediction of your model may not be a legal move!\n",
    "      features = self.encoding(game_state)\n",
    "      actions = self.model(features)\n",
    "      actions = torch.argsort(actions,descending=True)\n",
    "      #check to see if the action is a legal action at the current stateac\n",
    "      \n",
    "      for action in actions:\n",
    "          if(action.item() in self.search_problem.get_available_actions(game_state)):\n",
    "              if(action.item() != self.board_size**2 ):\n",
    "                return action.item()\n",
    "          \n",
    "      return game_state.size**2\n",
    "      # Note, you may want to force your policy not to pass their turn unless necessary\n",
    "      #assert action in self.search_problem.get_available_actions(game_state)\n",
    "      \n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return \"Policy Agent\"\n",
    "    \n",
    "def create_policy_agent_from_model():\n",
    "    \"\"\"\n",
    "    Create agent object from saved model. This (or other methods like this) will be how your agents will be created in gradescope and in the final tournament.    \n",
    "    \"\"\"\n",
    "\n",
    "    model_path = \"policy_model.pt\"\n",
    "    agent = PolicyAgent(GoProblem(size=5), model_path)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "8j6tGngt_LVu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Agent Policy Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 79.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1: Policy Agent Score: -35.0\n",
      "Agent 2: GreedyAgent + Simple Heuristic Score: 35.0\n",
      "Agent 1: Policy Agent Score with Black (first move): -29.0\n",
      "Agent 2: GreedyAgent + Simple Heuristic Score with Black (first move): 6.0\n",
      "Agent 1: Policy Agent Average Duration: 8.201638857523602e-05\n",
      "Agent 2: GreedyAgent + Simple Heuristic Average Duration: 0.0001570437590281169\n",
      "Agent 1: Policy Agent Longest Duration: 0.0008039474487304688\n",
      "Agent 2: GreedyAgent + Simple Heuristic Longest Duration: 0.0005230903625488281\n",
      "Agent 1: Policy Agent Average Time Remaining: 39.86796121835709\n",
      "Agent 2: GreedyAgent + Simple Heuristic Average Time Remaining: 39.86610211610796\n",
      "Agent 1: Policy Agent Min Time Remaining: 26.998926639556885\n",
      "Agent 2: GreedyAgent + Simple Heuristic Min Time Remaining: 26.997395992279053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-35.0, 35.0)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_agent = PolicyAgent(GoProblem(size=5),\"policy_model.pt\")\n",
    "learned_agent = create_value_agent_from_model(value_net)\n",
    "print(\"Policy Agent\", policy_agent)\n",
    "run_many(policy_agent,GreedyAgent(), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z2azdVNBxYS"
   },
   "source": [
    "# Submitting\n",
    "\n",
    "After you've completed all the tasks in this notebook, you'll want to add your agents to your agents.py file. You'll want to copy the necessary function and class definitions for PolicyAgent, GoProblemLearnedHeuristic, PolicyNetwork, ValueNetwork, and any other methods you referenced. Your agents will ultimately be tested on gradescope by calling create_value_agent_from_model or by create_policy_agent_from_model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
